{% extends 'index.html' %}


{% block navbar %}
    <li class="nav-item">
        <a class="nav-link" href="{{url_for('home')}}">Home</a>
    </li>
    <li class="nav-item">
        <a class="nav-link" href="{{url_for('about')}}"style="color: red;">About</a>
    </li>
    <li class="nav-item">
        <a class="nav-link" href="{{url_for('mic')}}">Speech to Text</a>
    </li>
    <li class="nav-item">
        <a class="nav-link" href="{{url_for('prediction')}}">Gesture Recoginition</a>
    </li>
    <li class="nav-item">
        <a class="nav-link" href="{{ url_for('my_recordings') }}">My Recordings</a>
      </li>
    <li class="nav-item">
        <a class="nav-link" href="{{url_for('index')}}">LogOut</a>
    </li>
{% endblock %}


{% block content %}
    <section class="about_section layout_padding">
        <div class="container-fluid">
            <div class="row">
                <div class="col-12 ml-auto pr-0">
                    <div class="about_container">
                        <div class="row">
                            <center>
                                <div class="col-10" style="background-color: rgba(0, 0, 0, 0.548); margin-top: -100px;">
                                    <div class="detail-box">
                                        <div class="heading_container">
                                            <h2><br>About</h2>
                                        </div>
                                        <p style="text-align: justify;">
                                            This project focuses on creating an accessible system by integrating two key modules: ASL Alphabet Recognition and Speech-to-Text Conversion.
                                             Module 1: ASL Alphabet Recognition
                                            The first module recognizes American Sign Language (ASL) alphabets through hand gestures. Using a camera for real-time video input, the system processes the hand gestures and converts them into the corresponding text characters. Computer vision techniques, such as OpenCV and deep learning models (e.g., CNNs), are used for gesture recognition and classification.                                           
                                            Module 2: Speech-to-Text Conversion
                                            The second module converts spoken language into text. It listens to audio input from a microphone, processes the speech using speech recognition technologies DeepSpeech, and outputs the transcription as text. This module helps individuals with speech impairments communicate effectively through text.                                           
                                            Integration and Applications
                                            By combining both modules, this project enables seamless communication for individuals with hearing or speech impairments. Users can switch between ASL gestures and speech, facilitating inclusive interactions. Potential applications include accurate translation in public spaces, healthcare, and education, promoting accessibility and bridging communication gaps.                                           
                                            This project is a step towards enhancing communication for people with disabilities, offering innovative solutions to improve everyday interactions.
                                        </p><br>
                                    </div>
                                </div>
                            </center>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
{% endblock %}